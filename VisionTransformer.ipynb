{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 We will implement Vision Transformer (ViT) following the below steps:\n",
    "\n",
    "1. Patch embedding \n",
    "<br> The input images are 2D images, and the input of the self-attention os 1D (sequences). So we need to divide every input image into multiple patches and then embed them into sequences.\n",
    "2. Mutil-Head Attention\n",
    "<br> The embedded sequences are passed into a self-attention mechanism. In this process, the attention or dependence between sequences is established\n",
    "3. Build the Transformer class that contains the Mutil-Head Attention and other activation functions\n",
    "4. Build the Encoder class that contain multiple Transformer blocks\n",
    "5. Finally, we build the ViT model based on the Patch embedding and Encoder.\n",
    "\n",
    "### 2. We test our model on the Cifar 10 and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this tutorial, we use einops for matrix rearangement of mutiplication\n",
    "# pip install einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from torch import nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "import os, csv, time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import collections\n",
    "try:\n",
    "    from collections import OrderedDict\n",
    "except ImportError:\n",
    "    OrderedDict = dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Patch embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patching(nn.Module):\n",
    "    def __init__(self, in_channels= 3, img_size = 224, patch_size= 16, embed_size = 768):\n",
    "      # embed_size = in_channels x patchsize**2\n",
    "        super(Patching, self).__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.num_path = int(img_size//patch_size)**2\n",
    "        self.projection = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1 = self.patch_size, p2 = self.patch_size),\n",
    "            nn.LayerNorm(embed_size),\n",
    "            nn.Linear(embed_size, embed_size ),\n",
    "            nn.LayerNorm(embed_size ))\n",
    "\n",
    "        self.class_token = nn.Parameter(torch.randn(1,1, embed_size))  \n",
    "        self.pos_embedding = nn.Parameter(torch.randn(self.num_path + 1, embed_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, h, w = x.shape\n",
    "        x = self.projection(x)\n",
    "        class_token = repeat(self.class_token, '() n e -> b n e', b=b)\n",
    "\n",
    "        x = torch.cat([class_token, x], dim=1)\n",
    "        # add position embedding\n",
    "        x += self.pos_embedding\n",
    "\n",
    "        return x\n",
    "\n",
    "# x = torch.rand(1,3,224,224)   \n",
    "# Patching()(x).shape\n",
    "# model = Patching()\n",
    "# summary(model, (3,224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "\n",
    "  def __init__(self, embed_size, num_heads, dropout = 0):\n",
    "\n",
    "    super(MultiHeadAttention, self).__init__()\n",
    "\n",
    "    self.emb_size = embed_size\n",
    "    self.num_heads = num_heads\n",
    "    self.head_size = embed_size//num_heads\n",
    "\n",
    "    assert embed_size % num_heads == 0, \"embed_size % num_heads should be zero.\"\n",
    "\n",
    "    # Determin Wq, Qk and Qv in Attention\n",
    "    self.keys = nn.Linear(embed_size, self.head_size*num_heads) # (Wk matrix) \n",
    "    self.queries = nn.Linear(embed_size,  self.head_size*num_heads) # (Wq matrix) \n",
    "    self.values = nn.Linear(embed_size,  self.head_size*num_heads) # (Wv matrix) \n",
    "\n",
    "    self.att_drop = nn.Dropout(dropout)\n",
    "    self.dense = nn.Linear(embed_size, embed_size)\n",
    "\n",
    "  def forward(self, x):     \n",
    "    # x.shape = [Batchsize (B) x num_patch (np) x embed_size (ez)] \n",
    "    batch_size, np, ez = x.shape\n",
    "    key = self.keys(x)            # [B x (np x ez)] x [ez x ez] = [B x np x ez] \n",
    "    query = self.queries(x)       # [B x (np x ez)] x [ez x ez] = [B x np x ez]\n",
    "    value = self.values(x)        # [B x (np x ez)] x [ez x ez] = [B x np x ez]\n",
    "\n",
    "    # split key, query and value in many num_heads\n",
    "    key = key.view(batch_size, -1, self.num_heads, self.head_size)      # [B x np x h x s]\n",
    "    query = query.view(batch_size, -1, self.num_heads, self.head_size)  # [B x np x h x s]\n",
    "    value = value.view(batch_size, -1, self.num_heads, self.head_size)  # [B x np x h x s]\n",
    "\n",
    "    key = key.permute(2, 0, 1 ,3).contiguous().view(batch_size * self.num_heads, -1, self.head_size) # [(Bh) x np x s]\n",
    "    query = query.permute(2, 0, 1 ,3).contiguous().view(batch_size * self.num_heads, -1, self.head_size) # [(Bh) x np x s]\n",
    "    value = value.permute(2, 0, 1 ,3).contiguous().view(batch_size * self.num_heads, -1, self.head_size) # [(Bh) x np x s]\n",
    "    # Q x K matrix\n",
    "    score = torch.bmm(query, key.transpose(1, 2)) / math.sqrt(self.head_size)\n",
    "    soft = F.softmax(score, -1)\n",
    "    context = torch.bmm(soft, value)\n",
    "    context = self.att_drop(context)\n",
    "    # Convert to the original size\n",
    "    context = context.view(self.num_heads, batch_size, -1, self.head_size) # [h x B x np x s]\n",
    "    context = context.permute(1, 2, 0, 3).contiguous().view(batch_size, -1, self.num_heads * self.head_size)\n",
    "\n",
    "    attention = self.dense(context)\n",
    "\n",
    "    return attention #  [Batchsize (B) x num_patch (np) x embed_size (ez)]\n",
    "  \n",
    "\n",
    "# x = torch.rand(1,4 ,32)\n",
    "# attention = MultiHeadAttention( embed_size=32, num_heads=2)\n",
    "# summary(attention, (4, 32))\n",
    "\n",
    "'''\n",
    "with embed_size=32, num_heads=2\n",
    "[1,4,32] x W (linear)---> [1,4,32] ---(devide by 2 heads)----> [1 2 4 16] shape of Q, K, V\n",
    "Soft = QxK [1 2 4 16] x [1 2 16 4].T = [1 2 4 4] \n",
    "attention = [1 2 4 4] x [1 2 4 16]---> [1 2 4 16] ---rearrange---> [1, 4, 32] ---(dense)---> [1, 4, 32]\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "  def __init__(self, embed_size, num_heads, expansion, dropout = 0):\n",
    "    super(TransformerBlock, self).__init__()\n",
    "\n",
    "    self.norm1 = nn.LayerNorm(embed_size)\n",
    "    self.mul_attention = MultiHeadAttention(embed_size,num_heads)\n",
    "    self.drop = nn.Dropout(dropout)\n",
    "    self.norm2 = nn.LayerNorm(embed_size)\n",
    "    self.mlp = nn.Sequential(nn.Linear(embed_size, expansion*embed_size),\n",
    "                            nn.GELU(),\n",
    "                            nn.Dropout(dropout),\n",
    "                            nn.Linear(expansion*embed_size, embed_size))\n",
    "                            \n",
    "  def forward(self, x):\n",
    "    out = x + self.drop(self.mul_attention(self.norm1(x)))\n",
    "    out = out + self.drop(self.mlp(self.norm2(out)))\n",
    "    return out\n",
    "\n",
    "# x = torch.rand(1,4 ,32)\n",
    "# block = TransformerBlock(embed_size =32, num_heads=2, expansion=2)\n",
    "# print(block(x).shape)\n",
    "# summary(block, (4, 32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self,embed_size, num_heads, expansion, dropout, depth):\n",
    "     super(Encoder, self).__init__()\n",
    "\n",
    "     layers: OrderedDict[str, nn.Module] = OrderedDict()\n",
    "\n",
    "     for i in range(depth):\n",
    "       layers[f\"encoder_layer_{i}\"] = TransformerBlock(embed_size, num_heads, expansion, dropout)\n",
    "     self.layers = nn.Sequential(layers)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.layers(x)\n",
    "\n",
    "# x = torch.rand(1,4 ,32)   \n",
    "# encoder = Encoder(embed_size=32, num_heads=2, expansion=2, dropout=0.2, depth=2)\n",
    "# print(encoder)\n",
    "# print(encoder(x).shape)\n",
    "# summary(encoder, (4, 32)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vision Transformer (ViT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIT(nn.Module):\n",
    "  def __init__(self,  in_channels= 3, img_size = 32, patch_size= 4, embed_size = 48, \n",
    "               num_heads = 2, expansion = 4, dropout= 0.2, depth = 4, num_classes = 10):\n",
    "    # embed_size = in_channels x patchsize**2\n",
    "    super(VIT, self).__init__()\n",
    "    self.path_embedding = Patching(in_channels, img_size, patch_size, embed_size) \n",
    "    self.encoder = Encoder(embed_size, num_heads, expansion, dropout, depth)\n",
    "    self.num_class = nn.Sequential(Reduce('b n e -> b e', reduction='mean'), \n",
    "                                   nn.LayerNorm(embed_size),\n",
    "                                   nn.Linear(embed_size, num_classes))\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.path_embedding(x)\n",
    "    x = self.encoder(x)\n",
    "    x = self.num_class(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "model = VIT(in_channels= 3, img_size = 32, patch_size= 4,\n",
    "            embed_size = 48, num_heads = 2, expansion = 4,\n",
    "            dropout= 0.2, depth = 4, num_classes = 10)\n",
    "\n",
    "x = torch.rand(1,3,32, 32)\n",
    "summary(model, (3,32, 32))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By now, we have built the ViT completely. So let's test the model on the Cifar 10 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using: {device} to train the models\")\n",
    "\n",
    "# model = model.to(device)\n",
    "net = VIT().to(device)\n",
    "final_epoch = 2000\n",
    "batch_size = 32\n",
    "\n",
    "# Dataset\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), \n",
    "                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "test_dataloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, label = next(iter(train_dataloader))\n",
    "print(image.shape, label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_dataloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "# print labels\n",
    "print(' '.join(f'{classes[labels[j]]:5s}' for j in range(batch_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(3):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print(f'[Epoch: {epoch + 1}, Number of images: {i + 1:5d}] loss: {running_loss / 2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model and implement on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = './cifar_net.pth'\n",
    "torch.save(net.state_dict(), PATH)\n",
    "dataiter = iter(test_dataloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# print images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "print('GroundTruth: ', ' '.join(f'{classes[labels[j]]:5s}' for j in range(4)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = VIT()\n",
    "net.load_state_dict(torch.load(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = net(images)\n",
    "_, predicted = torch.max(outputs, 1)\n",
    "\n",
    "print('Predicted: ', '  '.join(f'{classes[predicted[j]]:5s}'\n",
    "                              for j in range(4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on all test dataset images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        # calculate outputs by running images through the network\n",
    "        outputs = net(images)\n",
    "        # the class with the highest energy is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_pred = {classname: 0 for classname in classes}\n",
    "total_pred = {classname: 0 for classname in classes}\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_dataloader:\n",
    "        images, labels = data\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[classes[label]] += 1\n",
    "            total_pred[classes[label]] += 1\n",
    "\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {classname:5s} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
